{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrumpSpeech.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19qpj2bEmApxUf-Bh7SmF9qezSzSjU7Sj",
      "authorship_tag": "ABX9TyNUzx30HI9FdHrjAl5T4hct",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/hkalexling/6bfe2f3d23cbb5ceffadcaecb739b7f8/trumpspeech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLxDOY2ViHC4"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XHtByOZ7aOm"
      },
      "source": [
        "CHECKPOINT='drive/My Drive/Colab/trump-speech/1.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U29iTPwriUp-"
      },
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "# Download and pre-process the dataset\n",
        "\n",
        "data = requests.get('https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt').text\n",
        "data = re.sub('SPEECH [0-9]+|\\r|\\ufeff', '', data)\n",
        "data = re.sub('\\n', ' ', data)\n",
        "data = re.sub('[ ]+', ' ', data)\n",
        "\n",
        "texts = [t.strip() for t in data.split('. ')]\n",
        "print(texts)\n",
        "print(len(texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fDfhx6Fm3QS"
      },
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "word_count = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(word_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr2AGkBxnAPm"
      },
      "source": [
        "# Generate n-grams as training set\n",
        "n_grams = []\n",
        "\n",
        "seqs = tokenizer.texts_to_sequences(texts)\n",
        "for seq in seqs:\n",
        "  for i in range(1, len(seq)):\n",
        "    n_grams.append(seq[:i+1])\n",
        "\n",
        "max_len = max([len(n) for n in n_grams])\n",
        "n_grams = np.array(pad_sequences(n_grams, maxlen=max_len, padding='pre'))\n",
        "\n",
        "print(n_grams.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22-W9YKeoJck"
      },
      "source": [
        "X, y = n_grams[:,:-1], n_grams[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh6OvBHzoSYb"
      },
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "    \n",
        "model.add(Embedding(word_count, 10, input_length=max_len-1))\n",
        "\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "    \n",
        "model.add(Dense(word_count, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam') \n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FINsej3xooJK"
      },
      "source": [
        "import tensorflow as tf\n",
        "cb = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINT, save_weights_only=True, verbose=True)\n",
        "\n",
        "try:\n",
        "  model.load_weights(CHECKPOINT)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "model.fit(X, y, batch_size=256, epochs=200, verbose=True, callbacks=[cb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmE3GzOxp1Nr"
      },
      "source": [
        "# Test the model\n",
        "model.load_weights(CHECKPOINT)\n",
        "\n",
        "txt = \"Make\"\n",
        "\n",
        "for _ in range(20):\n",
        "    tokens = tokenizer.texts_to_sequences([txt])[0]\n",
        "    tokens = pad_sequences([tokens], maxlen=max_len-1, padding='pre')  \n",
        "    index = np.random.choice(word_count, p=model.predict(tokens).reshape((word_count)))\n",
        "    \n",
        "    txt += \" \" + tokenizer.index_word[index]\n",
        "    \n",
        "print(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZdrUz0TV5dV"
      },
      "source": [
        "# Save the model\n",
        "\n",
        "model.save('drive/My Drive/Colab/trump-speech/model')\n",
        "!saved_model_cli show --dir 'drive/My Drive/Colab/trump-speech/model' --all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ6QYyanbKnv"
      },
      "source": [
        " # Export the word-index mapping\n",
        " \n",
        " import json\n",
        "\n",
        " with open('word_index.json', 'w+') as f:\n",
        "   json.dump(tokenizer.word_index, f)\n",
        "\n",
        " with open('index_word.json', 'w+') as f:\n",
        "    json.dump(tokenizer.index_word, f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}